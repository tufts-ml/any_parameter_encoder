# Goals

Sanity check the ELBO computation of the current APE code.

Compare the following:

* -1.33: baseline of a general unigram model
* -0.514: manual calculation of the ELBO
* -1.62: calculation of the ELBO with SVI

# Results from baselines

```
(ape) [mhughe02@omega001 202006_compare_manual_to_pyro_elbo]$ python calc_manual_elbo.py 
--alpha 0.1
--q_stddev 0.1
--n_mc_samples 5
--seed 42
5 MC samples of ELBO-per-token:
[-0.514 -0.514 -0.514 -0.514 -0.514]

Unigram-logpmf-per-token:
[-1.326]
```
```
(ape) [mhughe02@omega001 202006_compare_manual_to_pyro_elbo]$ bash try_svi.sh 
/cluster/tufts/hugheslab/code/any_parameter_encoder /cluster/tufts/hugheslab/code/any_parameter_encoder/workflows/202006_compare_manual_to_pyro_elbo
CUDA: False
num_words 500585.0
svi after 10 trials
log pmf per token:  -1.61 (min  -1.62, max  -1.61)
runtime: 0.0000 sec (min 0.0000, max 0.0000)
```



# Data

1000 documents from a "toy bars" setting.

True topics are generated by:

* beta_KV : 10 topics each with a uniform distribution over one horiz/vertical bar
* theta_KV : drawn from Dir(5 * beta_KV)


# Baseline Unigram model

$$
p( x_n | p) = \text{Multinomial}( T_n, p_1, \ldots p_V), \quad p \in \Delta^V
$$

we fit $p$ via maximum likelihood on the training set.

# Target model: Logistic-Normal Topic model

## Generative Model

$$
p(h) &= \mathcal{N}( \mu_0, \Sigma_0), \quad h \in \mathbb{R}^K
\\
p(x_n | h_n) &= \text{Multinomial}( T_n, \sum_k \pi_k(h) \theta ), \quad x_n \in \mathbb{Z}_+^V
$$

## Prerequisites

Install ape conda environment

```
conda env create -f ape-linux64.yml
```



